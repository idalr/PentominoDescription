# PentominoDescription

This repository was part of the CogSys Master's project module (PM2: Project in Machine Learning; Multimodal Dialogue in Human-Robot Interaction), which I took in the Summer Semester 2022 at the University of Potsdam, Germany.

The repository only contains the sub-tasks I was responsible for, which was to preprocess textual data and build classifiers, which match a the textual description to a pentomino piece. The accuracy scores, as a result, of two different types of classifier (NB and LSTM) are shown.

## Project description

I am a part of Group D: Language and Vision; our goal is to build a multimodal model that correctly detect a pentomino piece that human participant describes verbally in a real-time visual scene, and send the pick-up coordinate to the robot arm. The overview of the project is as follows:
1. Corpora
   - TakeCV
   - Survey
   - Augmented data
2. Experiments
   - Vision
     - Fast R-CNN
     - YOLO
     - Grabbing Point
   - Language
     - Naive Bayes 
     - CNNs
     - LSTMs
   - Combining LV Models

The project consists of three notebooks without data uploaded.
